<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 6.0.1 | Copyright Dean Attali 2023 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  

  <title>An Introduction to Preference-Based RL | Padmanaba Srinivasan</title>

  
  
  <meta name="author" content="Padmanaba Srinivasan">
  

  <meta name="description" content="Intro In Reinforcement Learning (RL), an agent navigates through an environment and learns to maximize a discounted sum of scalar rewards produced as a property of the environment. In practice, however, no environment decides to provide a reward signal – such signals are the product of human design. At its...">

  

  

  

  

  

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Padmanaba Srinivasan">
  <meta property="og:title" content="An Introduction to Preference-Based RL | Padmanaba Srinivasan">
  <meta property="og:description" content="Intro In Reinforcement Learning (RL), an agent navigates through an environment and learns to maximize a discounted sum of scalar rewards produced as a property of the environment. In practice, however, no environment decides to provide a reward signal – such signals are the product of human design. At its...">

  
  <meta property="og:image" content="http://localhost:4000/assets/img/path.jpg">
  

  
  <meta property="og:type" content="article">
  
  <meta property="og:article:author" content="Padmanaba Srinivasan">
  
  <meta property="og:article:published_time" content="2024-02-22T00:00:00-05:00">
  <meta property="og:url" content="http://localhost:4000/2024-02-22-tech-blog-An-Introduction-to-Preference-Based-RL/">
  <link rel="canonical" href="http://localhost:4000/2024-02-22-tech-blog-An-Introduction-to-Preference-Based-RL/">
  

  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="An Introduction to Preference-Based RL | Padmanaba Srinivasan">
  <meta property="twitter:description" content="Intro In Reinforcement Learning (RL), an agent navigates through an environment and learns to maximize a discounted sum of scalar rewards produced as a property of the environment. In practice, however, no environment decides to provide a reward signal – such signals are the product of human design. At its...">

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/path.jpg">
  

  


  

  
  

  

</head>


<body>
  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4000/">Padmanaba Srinivasan</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/cv/CV.pdf">CV</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://scholar.google.com/citations?user=_yyqhBEAAAAJ&hl=en">Publications</a>
          </li></ul>
  </div>

  

  

</nav>





  


  <div id="header-big-imgs" data-num-img=1
    
    
    
    
      data-img-src-1="http://localhost:4000/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">
<div class="intro-header  big-img ">
  
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>An Introduction to Preference-Based RL</h1>
          
          
           
            
            <span class="post-meta">Posted on February 22, 2024</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
  
  <span class='img-desc'></span>
</div>



</header>


<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<main class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <div class="blog-post">
        <h1 id="intro">Intro</h1>

<p>In Reinforcement Learning (RL), an agent navigates through an environment and learns to maximize a discounted sum of scalar rewards produced as a property of the environment. In practice, however, no environment <em>decides</em> to provide a reward signal – such signals are the product of human design.</p>

<p>At its simplest, a reward might consist of a simple binary signal that indicates when a task has been successfully completed at termination. Such sparse feedback may pose challenges to learning, resulting in the need for <a href="https://www.cdf.toronto.edu/~csc2542h/fall/material/csc2542f16_reward_shaping.pdf">reward shaping</a>. More considerate reward design can improve learning efficiency. However, this requires far more effort in design to ensure that the reward signal is informative and robust to <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/3d719fee332caa23d5038b8a90e81796-Abstract-Conference.html">reward hacking</a> where flaws in the reward signal are exploited by the agent, resulting in undesireable behavior – see <a href="https://openai.com/index/faulty-reward-functions/">this</a> for an illustrated example.</p>

<p>Clearly, for many tasks constructing an informative reward function is challenging because without the ability to explicitly communicate which behaviors are preferred, the agent can exploit the reward function. The discrepancy between human preferences and reward function-implied behavior leads to <em>misalignment</em> between the desired and actual behavior of RL-trained policies (see <a href="https://www.goodreads.com/book/show/20527133-superintelligence">Bostrom</a> and <a href="https://www.scientificamerican.com/article/should-we-fear-supersmart-robots/#:~:text=The%20machine's%20purpose%20must%20be,way%20it%20sidesteps%20Wiener's%20problem.">Russell</a>).</p>

<p><a href="https://arxiv.org/abs/1706.03741">Reinforcement Learning from Human Feedback (RLHF)</a> is one paradigm designed to align RL policy performance with human preferences; the key difference between RLHF and typical RL is in how human preferences are incorporated and iteratively used to refine a policy to produce human-preferred results.</p>

<p>RLHF underpins many successful applications: first and foremost is to finetune pretrained large language models (LLMs) such as <a href="https://openai.com/index/instruction-following/">InstructGPT</a> (<a href="https://huyenchip.com/2023/05/02/rlhf.html">this blog</a> does a great job of describing RLHF in LLMs very well),</p>

      </div>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#tech">tech</a>
          
            <a href="/tags#machine learning">machine learning</a>
          
            <a href="/tags#reinforcement learning">reinforcement learning</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2024-02-22-tech-blog-A-Overview-of-Model-Based-Offline-RL/" data-toggle="tooltip" data-placement="top" title="An Overview of Model-Based Offline RL Methods">
            <i class="fas fa-arrow-left" alt="Previous Post"></i>
            <span class="d-none d-sm-inline-block">Previous Post</span>
          </a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2024-02-22-tech-blog-Some-Interesting-Offline-RL-Methods-Early-2024/" data-toggle="tooltip" data-placement="top" title="Some Interesting Offline RL Methods (Early 2024)">
            <span class="d-none d-sm-inline-block">Next Post</span>
            <i class="fas fa-arrow-right" alt="Next Post"></i>
          </a>
        </li>
        
      </ul>
      
  
  
  

  


  

  



    </div>
  </div>
</main>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      
<ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:padmanabasrinivasan@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/dangerbot3pic" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/padmanaba-srinivasan" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://scholar.google.com/citations?user=citations?user=_yyqhBEAAAAJ&hl=en" title="Google Scholar">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Google Scholar</span>
    </a>
  </li></ul>


      
      <p class="copyright text-muted">
      
        Padmanaba Srinivasan
        &nbsp;&bull;&nbsp;
      
      2025

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4000/">padmanabasrinivasan.github.io</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
