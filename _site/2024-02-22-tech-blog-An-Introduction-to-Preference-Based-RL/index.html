<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2020 -->
  <head>
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>An Introduction to Preference-Based RL</title>
  
  
  <meta name="author" content="Padmanaba Srinivasan">
  
  
  

  <link rel="alternate" type="application/rss+xml" title="Padmanaba Srinivasan - Computer Science PhD Student" href="http://localhost:4000/feed.xml">

  

  

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-172123502-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-172123502-1');
</script>



  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/main.css">
    
  

  

  

  <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="An Introduction to Preference-Based RL">
  

   
  <meta property="og:description" content="Intro In Reinforcement Learning (RL), an agent navigates through an environment and learns to maximize a discounted sum of scalar rewards produced as a property of the environment. In practice, however, no environment decides to provide a reward signal – such signals are the product of human design. At its...">
  


  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Padmanaba Srinivasan">
  <meta property="og:article:published_time" content="2024-02-22T00:00:00+00:00">
  

  
  <meta property="og:url" content="http://localhost:4000/2024-02-22-tech-blog-An-Introduction-to-Preference-Based-RL/">
  <link rel="canonical" href="http://localhost:4000/2024-02-22-tech-blog-An-Introduction-to-Preference-Based-RL/">
  
  

  
  <meta property="og:image" content="http://localhost:4000/assets/img/path.jpg">
  


  <!-- Twitter summary cards -->
  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  
  <meta name="twitter:title" content="An Introduction to Preference-Based RL">
  

  
  <meta name="twitter:description" content="Intro In Reinforcement Learning (RL), an agent navigates through an environment and learns to maximize a discounted sum of scalar rewards produced as a property of the environment. In practice, however, no environment decides to provide a reward signal – such signals are the product of human design. At its...">
  

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/path.jpg">
  

  

  

</head>


  <body>

    

  
    <nav class="navbar navbar-expand-md navbar-light fixed-top navbar-custom "><a class="navbar-brand" href="http://localhost:4000/">Padmanaba Srinivasan</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/cv/CV.pdf">CV/Resume</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/techblog">Blog</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://scholar.google.com/citations?user=_yyqhBEAAAAJ&hl=en">Publications</a>
          </li></ul>
  </div>

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navbar avatar" class="avatar-img" src="/assets/img/park.jpg" />
        </a>
      </div>
    </div>
  

</nav>


    <!-- TODO this file has become a mess, refactor it -->






  <div id="header-big-imgs" data-num-img=1
    
    
    
      
      data-img-src-1="http://localhost:4000/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">

<div class="big-img intro-header">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>An Introduction to Preference-Based RL</h1>
      
      
      
          <span class="post-meta">Posted on 22 February, 2024</span>
          
      
        </div>
      </div>
    </div>
  </div>
  <span class='img-desc'></span>
</div>

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>An Introduction to Preference-Based RL</h1>
      
      
      
          <span class="post-meta">Posted on 22 February, 2024</span>
          
      
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class="container-md">
  <div class="row">
    <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">

      

      <article role="main" class="blog-post">
        <h1 id="intro">Intro</h1>

<p>In Reinforcement Learning (RL), an agent navigates through an environment and learns to maximize a discounted sum of scalar rewards produced as a property of the environment. In practice, however, no environment <em>decides</em> to provide a reward signal – such signals are the product of human design.</p>

<p>At its simplest, a reward might consist of a simple binary signal that indicates when a task has been successfully completed at termination. Such sparse feedback may pose challenges to learning, resulting in the need for <a href="https://www.cdf.toronto.edu/~csc2542h/fall/material/csc2542f16_reward_shaping.pdf">reward shaping</a>. More considerate reward design can improve learning efficiency. However, this requires far more effort in design to ensure that the reward signal is informative and robust to <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/3d719fee332caa23d5038b8a90e81796-Abstract-Conference.html">reward hacking</a> where flaws in the reward signal are exploited by the agent, resulting in undesireable behavior – see <a href="https://openai.com/index/faulty-reward-functions/">this</a> for an illustrated example.</p>

<p>Clearly, for many tasks constructing an informative reward function is challenging because without the ability to explicitly communicate which behaviors are preferred, the agent can exploit the reward function. The discrepancy between human preferences and reward function-implied behavior leads to <em>misalignment</em> between the desired and actual behavior of RL-trained policies (see <a href="https://www.goodreads.com/book/show/20527133-superintelligence">Bostrom</a> and <a href="https://www.scientificamerican.com/article/should-we-fear-supersmart-robots/#:~:text=The%20machine's%20purpose%20must%20be,way%20it%20sidesteps%20Wiener's%20problem.">Russell</a>).</p>

<p><a href="https://arxiv.org/abs/1706.03741">Reinforcement Learning from Human Feedback (RLHF)</a> is one paradigm designed to align RL policy performance with human preferences; the key difference between RLHF and typical RL is in how human preferences are incorporated and iteratively used to refine a policy to produce human-preferred results.</p>

<p>RLHF underpins many successful applications: first and foremost is to finetune pretrained large language models (LLMs) such as <a href="https://openai.com/index/instruction-following/">InstructGPT</a> (<a href="https://huyenchip.com/2023/05/02/rlhf.html">this blog</a> does a great job of describing RLHF in LLMs very well),</p>

      </article>

      
        <div class="blog-tags">
          Tags:
          
          
            <a href="/tags#tech">tech</a>
          
            <a href="/tags#machine learning">machine learning</a>
          
            <a href="/tags#reinforcement learning">reinforcement learning</a>
          
          
        </div>
      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
    <a href="https://twitter.com/intent/tweet?text=An+Introduction+to+Preference-Based+RL&url=http%3A%2F%2Flocalhost%3A4000%2F2024-02-22-tech-blog-An-Introduction-to-Preference-Based-RL%2F"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fab fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  
    <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2024-02-22-tech-blog-An-Introduction-to-Preference-Based-RL%2F"
      class="btn btn-social-icon btn-facebook" title="Share on Facebook">
      <span class="fab fa-fw fa-facebook" aria-hidden="true"></span>
      <span class="sr-only">Facebook</span>
    </a>
  

  
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2024-02-22-tech-blog-An-Introduction-to-Preference-Based-RL%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fab fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

  

</section>



      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2024-02-22-tech-blog-A-Overview-of-Model-Based-Offline-RL/" data-toggle="tooltip" data-placement="top" title="An Overview of Model-Based Offline RL Methods">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2024-02-22-tech-blog-Some-Interesting-Offline-RL-Methods-Early-2024/" data-toggle="tooltip" data-placement="top" title="Some Interesting Offline RL Methods (Early 2024)">Next Post &rarr;</a>
        </li>
        
      </ul>
              
  
  
  

  



    </div>
  </div>
</div>


    <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:padmanaba.srinivasan16@imperial.ac.uk" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/dangerbot3pic" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/padmanaba-srinivasan-b67bb1137" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://scholar.google.com/citations?user=_yyqhBEAAAAJ&hl=en" title="Google Scholar">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Google Scholar</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Padmanaba Srinivasan
        &nbsp;&bull;&nbsp;
      
      2024

      

      
      </p>
      <!-- Please don't remove this, keep my open source work credited :) -->
      <p class="theme-by text-muted">
        Theme by
        <a href="https://beautifuljekyll.com">beautiful-jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>

  
    
  
    
  <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/main.js"></script>
    
  






  
  </body>
</html>
