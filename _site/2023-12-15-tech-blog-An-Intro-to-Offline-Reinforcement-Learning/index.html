<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 6.0.1 | Copyright Dean Attali 2023 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  

  <title>An Intro to Offline Reinforcement Learning | Padmanaba Srinivasan</title>

  
  
  <meta name="author" content="Padmanaba Srinivasan">
  

  <meta name="description" content="What is Reinforcement Learning? Say you have bought a table from a certain Swedish retailer famous for both their fairly priced furnite, and meatballs, but have lost the instructions to assemble it. A trivial, albeit cumbersome approach would be to just try putting different parts together and to make it...">

  

  

  

  

  

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Padmanaba Srinivasan">
  <meta property="og:title" content="An Intro to Offline Reinforcement Learning | Padmanaba Srinivasan">
  <meta property="og:description" content="What is Reinforcement Learning? Say you have bought a table from a certain Swedish retailer famous for both their fairly priced furnite, and meatballs, but have lost the instructions to assemble it. A trivial, albeit cumbersome approach would be to just try putting different parts together and to make it...">

  
  <meta property="og:image" content="http://localhost:4000/assets/img/path.jpg">
  

  
  <meta property="og:type" content="article">
  
  <meta property="og:article:author" content="Padmanaba Srinivasan">
  
  <meta property="og:article:published_time" content="2023-12-15T00:00:00-05:00">
  <meta property="og:url" content="http://localhost:4000/2023-12-15-tech-blog-An-Intro-to-Offline-Reinforcement-Learning/">
  <link rel="canonical" href="http://localhost:4000/2023-12-15-tech-blog-An-Intro-to-Offline-Reinforcement-Learning/">
  

  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="An Intro to Offline Reinforcement Learning | Padmanaba Srinivasan">
  <meta property="twitter:description" content="What is Reinforcement Learning? Say you have bought a table from a certain Swedish retailer famous for both their fairly priced furnite, and meatballs, but have lost the instructions to assemble it. A trivial, albeit cumbersome approach would be to just try putting different parts together and to make it...">

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/path.jpg">
  

  


  

  
  

  

</head>


<body>
  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4000/">Padmanaba Srinivasan</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/cv/CV.pdf">CV</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://scholar.google.com/citations?user=_yyqhBEAAAAJ&hl=en">Publications</a>
          </li></ul>
  </div>

  

  

</nav>





  


  <div id="header-big-imgs" data-num-img=1
    
    
    
    
      data-img-src-1="http://localhost:4000/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">
<div class="intro-header  big-img ">
  
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>An Intro to Offline Reinforcement Learning</h1>
          
          
           
            
            <span class="post-meta">Posted on December 15, 2023</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
  
  <span class='img-desc'></span>
</div>



</header>


<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<main class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <div class="blog-post">
        <h2 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2>

<p>Say you have bought a table from a certain Swedish retailer famous for both their fairly priced furnite, and meatballs, but have lost the instructions to assemble it. A trivial, albeit cumbersome approach would be to just try putting different parts together and to make it look more and more like the image on the box. Through repeated attempts, you learn about how different parts fit together until mastery, at which point you are rewarded with a completed table.</p>

<p>This process of repetition and feedback describes the fundamental process of Reinforcement Learning (RL). More abstractly, in RL we aim to train an agent to maximize a reward by interacting with the environment. This is usually achieved by allowing the agent to take actions and receive a reward as feedback and repeating this process until it learns an <em>optimal policy</em> to solve the task.</p>

<p>Rephrasing all this more formally, the agent follows a policy \(\pi \in \mathcal{\Pi}\) in an <strong>environment</strong> by selecting an action \(a \in \mathcal{A}\) based on the current state \(s \in \mathcal{S}\). Once an action is taken, the environment produces a <strong>reward</strong> \(r \in \mathcal{R}\) which is used as feedback to the agent. By executing an action, the agent is able to effect a change in the environment that causes a transition to a new state \(s' \in \mathcal{S}\). The way in which environment dynamics work is defined by a <strong>model</strong>, which may be known or approximated allowing the use of model-based RL algorithms, or unknown in which case model-free RL algorithms must be used.</p>

<p>The agent’s goal is to learn a policy \(\pi(s)\) that maximizes the expected future reward and is taught to do so using a value function \(V(s)\) that predicts the expected future reward from that state following the policy \(\pi\).</p>

<h2 id="from-rl-to-offline-rl">From RL to Offline RL</h2>

<p>The agent needs to be trained on data collected by interacting with the environment over a series of time steps \(1, 2, ..., T\) expressed as a trajectory: \(S_1, A_1, r_1, S_2, A_2, r_2, ..., S_T, A_T, r_T\). <strong>On-Policy</strong> RL generates trajectories using the current policy \(\pi_k\) and learns from this to produce the next policy \(\pi_{k+1}\) to produce a sequence of policies that successively improve. <strong>Off-policy</strong> RL algorithms generate some trajectories using the current policy and
store trajectories from past policies in a replay buffer. Samples from the replay buffer therefore contain samples from a mixture of past policies and these are sampled to produce the next policy \(\pi_{k+1}\). The replay buffer can be updated periodically to add new experiences and remove old ones. In the <strong>Offline</strong> RL setting, the alrogithm is further constrained to using a fixed dataset of trajectories collected produced by (potentially unknown) behavioral policies, with no ability to interact with and explore the environment.</p>

<p>In the offline domain, only the values of the actions present in the dataset can be empirically learned. Standard off-policy RL algorithms have no way of evaluating and exploring the how good out-of-distribution (OOD) actions and so, the neural networks used to learn the value will extrapolate in OOD regions and <a href="https://offline-rl-neurips.github.io/pdf/41.pdf">overestimate their value</a> that the trained policy may select. In reality these actions may be quite poor, so when the policy is deployed in the environment it will execute these untrusted actions and perform poorly.</p>

<p>Clearly, in order to learn effectively online, an offline RL algorithm must both learn in a similar way to off-policy algorithms while also either:</p>

<ol>
  <li>Directly address the extrapolation problem and actively push down the values of OOD actions;</li>
  <li>Constrain the actor to select those actions present in the dataset.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Standard online deep learning methods have achieved incredible success in recent years, from playing <a href="https://en.wikipedia.org/wiki/AlphaGo">Go</a> to <a href="https://en.wikipedia.org/wiki/ChatGPT">conversational agents</a> which learn by interactng with the environment. In many domains though, it may be expensive or impossible to directly interact with the environment such as when training <a href="https://arxiv.org/abs/2110.07067">self-driving cars</a>, <a href="https://arxiv.org/abs/2105.01006">robotic surgeons</a> (including <a href="https://arxiv.org/pdf/2109.02323.pdf">safe methods</a> and other <a href="https://arxiv.org/abs/2002.03478">clinical</a> applications) and <a href="https://www.ijcai.org/proceedings/2020/0464.pdf">sports modelling</a>. Offline RL algorithms can learn from demonstrations produced by humans, that are potentially suboptimal and “stitch” together optimal trajectories from suboptimal data in a way that is <a href="https://arxiv.org/abs/2106.04895">sample efficient</a>.</p>

<p>I plan to do a more comprehensive <a href="https://arxiv.org/abs/2203.01387">review</a> of current offline RL methods as there have been many exciting developments in 2023 that have seen notable success; yet many of these methods also exhibit some disappointing trends in offline RL. I will cover some of of the most interesting papers of the year in my next post and my thoughts on each approach.</p>

      </div>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#tech">tech</a>
          
            <a href="/tags#machine learning">machine learning</a>
          
            <a href="/tags#reinforcement learning">reinforcement learning</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        
        <li class="page-item next">
          <a class="page-link" href="/2024-02-22-tech-blog-A-Overview-of-Model-Based-Offline-RL/" data-toggle="tooltip" data-placement="top" title="An Overview of Model-Based Offline RL Methods">
            <span class="d-none d-sm-inline-block">Next Post</span>
            <i class="fas fa-arrow-right" alt="Next Post"></i>
          </a>
        </li>
        
      </ul>
      
  
  
  

  


  

  



    </div>
  </div>
</main>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      
<ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:padmanabasrinivasan@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/dangerbot3pic" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/padmanaba-srinivasan" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://scholar.google.com/citations?user=citations?user=_yyqhBEAAAAJ&hl=en" title="Google Scholar">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Google Scholar</span>
    </a>
  </li></ul>


      
      <p class="copyright text-muted">
      
        Padmanaba Srinivasan
        &nbsp;&bull;&nbsp;
      
      2025

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4000/">padmanabasrinivasan.github.io</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
