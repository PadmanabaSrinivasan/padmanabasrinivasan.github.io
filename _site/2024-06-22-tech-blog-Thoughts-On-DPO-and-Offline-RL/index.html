<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 6.0.1 | Copyright Dean Attali 2023 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  

  <title>Thoughts on DPO and Offline RL | Padmanaba Srinivasan</title>

  
  
  <meta name="author" content="Padmanaba Srinivasan">
  

  <meta name="description" content="Direct Preference Optimization is all the rage now in LLMs, and rightly so! The derivation is neat (and very familiar to those experienced with reinforcement learning) and allows direct, preference-based finetuning of regression-trained LLMs without having to learn a reward model. In this post, I want to explore what implications...">

  

  

  

  

  

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Padmanaba Srinivasan">
  <meta property="og:title" content="Thoughts on DPO and Offline RL | Padmanaba Srinivasan">
  <meta property="og:description" content="Direct Preference Optimization is all the rage now in LLMs, and rightly so! The derivation is neat (and very familiar to those experienced with reinforcement learning) and allows direct, preference-based finetuning of regression-trained LLMs without having to learn a reward model. In this post, I want to explore what implications...">

  
  <meta property="og:image" content="http://localhost:4000/assets/img/path.jpg">
  

  
  <meta property="og:type" content="article">
  
  <meta property="og:article:author" content="Padmanaba Srinivasan">
  
  <meta property="og:article:published_time" content="2024-06-22T00:00:00-04:00">
  <meta property="og:url" content="http://localhost:4000/2024-06-22-tech-blog-Thoughts-On-DPO-and-Offline-RL/">
  <link rel="canonical" href="http://localhost:4000/2024-06-22-tech-blog-Thoughts-On-DPO-and-Offline-RL/">
  

  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="Thoughts on DPO and Offline RL | Padmanaba Srinivasan">
  <meta property="twitter:description" content="Direct Preference Optimization is all the rage now in LLMs, and rightly so! The derivation is neat (and very familiar to those experienced with reinforcement learning) and allows direct, preference-based finetuning of regression-trained LLMs without having to learn a reward model. In this post, I want to explore what implications...">

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/path.jpg">
  

  


  

  
  

  

</head>


<body>
  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4000/">Padmanaba Srinivasan</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/cv/CV.pdf">CV</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://scholar.google.com/citations?user=_yyqhBEAAAAJ&hl=en">Publications</a>
          </li></ul>
  </div>

  

  

</nav>





  


  <div id="header-big-imgs" data-num-img=1
    
    
    
    
      data-img-src-1="http://localhost:4000/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">
<div class="intro-header  big-img ">
  
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Thoughts on DPO and Offline RL</h1>
          
          
           
            
            <span class="post-meta">Posted on June 22, 2024</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
  
  <span class='img-desc'></span>
</div>



</header>


<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<main class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <div class="blog-post">
        <p><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization</a> is all the rage now in LLMs, and rightly so! The derivation is neat (and very familiar to those experienced with reinforcement learning) and allows direct, preference-based finetuning of regression-trained LLMs without having to learn a reward model.</p>

<p>In this post, I want to explore what implications a DPO-style training can offer to offline RL. To this end, I begin with the derivation.</p>

<p>Consider the following optimization problem:</p>

\[max_{\pi}\quad  \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [r(s, a)]\quad s.t.\quad D_{\text{KL}} [\pi (a | s) \mid\mid \pi_{\text{ref}} (a | s)] \leq \epsilon,\]

<p>which maximizes a reward function while constraining the KL–divergence between the current policy and a reference policy. Rewriting the objective using the Lagrangian multiplier \(\beta\), we get:</p>

\[max_{\pi}\quad  \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [r(s, a)] - \beta D_{\text{KL}} (\pi(a | s) \mid\mid \pi_{\text{ref}} (a | s))
    \\
    = max_{\pi}\quad \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [r(s, a) - \beta \log \frac{\pi(s | a)}{\pi_{\text{ref}} (s | a)}]
    \\
    = min_{\pi}\quad \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [\log \frac{\pi(s | a)}{\pi_{\text{ref}} (s | a)} - \frac{1}{\beta} r(s, a)]
    \\
    = min_{\pi}\quad \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi} [ \log \frac{\pi(a | s)}{\pi_{\text{ref}} (a | s) \exp (\frac{1}{\beta} r(s, a)) \frac{1}{Z(s)}}  - \log Z(s)],\]

<p>where</p>

\[Z(s) = \int_{\mathcal{A}} \pi_{\text{ref}} (a | s) \exp(\frac{1}{\beta} r(s, a)),\]

<p>is the state-dependent partition function.</p>

<p>The (estimated) reward can then be expressed as:</p>

\[r(s, a) = \beta \log \frac{\pi(a | s)}{\pi_{\text{ref}} (a | s)} + \beta \log Z(s)\]

<p>This poses a problem as \(Z(s)\) requires integrating over actions to compute – in practice this is intractable for continuous \(\mathcal{A}\). Most prior methods tends to ignore normalizing constant and attempts to approximate are rendered both <a href="https://arxiv.org/abs/2006.09359">more computationally expensive and less performant</a>.</p>

<p>The authors of DPO notice that rather than optimizing the objective directly, they can instead optimize the difference:</p>

\[r(s, a_1) - r(s, a_2) = (\beta \log \frac{\pi(a_1 | s)}{\pi_{\text{ref}} (a_1 | s)} + \beta \log Z(s)) - (\beta \log \frac{\pi(a_2 | s)}{\pi_{\text{ref}} (a_2 | s)} + \beta \log Z(s))
    \\
    = \beta (\log \frac{\pi(a_1 | s)}{\pi_{\text{ref}} (a_1 | s)} - \log \frac{\pi(a_2 | s)}{\pi_{\text{ref}} (a_2 | s)}),\]

<p>where the partition functions cancel out, resulting in a direct optimizion of log-probabilities. When training using preference-annotated data, the preference oracle denotes (absolute) paired preferences \(p(a_1 \succ a_2)\) that can be directly regressed via a <a href="https://en.wikipedia.org/wiki/Bradley–Terry_model">Bradley-Terry preference model</a> to yield the DPO objective.</p>

<p><strong>What does this mean for offline RL?</strong></p>

<p>First off, DPO is a <em>finetuning</em> process that requires the base policy \(\pi_{\text{ref}}\) to be a pretrained LLM – this can be a simple MLE-trained model – that is nudged to generate human-preferred sequences from an offline buffer. At no point is are samples drawn from \(\pi\) and instead, \(a_1\) and \(a_2\) are drawn from \(\pi_{\text{ref}}\). This is distinctly different from an RLHF-based approach that learns a reward model and requires humans to preference-rank samples drawn from a series of policies. Consequently, DPO is a far simpler and more stable approach to training LLMs.</p>

<p>Despite the <em>neatness</em> of DPO, by limiting the algorithm of offline finetuning based only on samples from a maximum likelihood model \(\pi_{\text{ref}}\) seems like it might limit the extent of improvement (or, to put it another way, human-preferred alignment) and this could be the case for many subsequent <a href="https://arxiv.org/pdf/2402.05749">offline refinement methods</a>.</p>

<p>Does DPO have any bearing on offline RL? Simply put, yes: several preference-inspired offline RL methods (for continuous control) have been proposed, such as (non-exhaustive):</p>

<ol>
  <li>Inverse-RL-inspired <a href="https://arxiv.org/abs/2305.15363">IPL</a> which learns a preference-based reward function and subsequently trains an RL policy.</li>
  <li><a href="https://arxiv.org/abs/2107.09251">OPAL</a> trains a policy on high-reward-seeking preferences between subtrajectories (following the behavioral policy).</li>
  <li><a href="https://arxiv.org/abs/2301.12842">DPPO</a> directly uses human-labelled datasets to train human-aligned policies.</li>
  <li><a href="https://arxiv.org/abs/2305.16217">OPPO</a> and <a href="https://arxiv.org/abs/2303.00957">PT</a> predict entire  to align with human-preferred trajectories.</li>
</ol>

<p>A commonality between these methods is their reliance on on-policy trajactories/value-estimation. A key driver of the development of offline RL methods is the ability to enable <em>stitching</em> together of suboptimal (behavior-policy-generated) trajectories to improve on the behavior policy. I have yet to come across any preference-inspired methods that can move beyond on the on-policy setting (and is perhaps why I haven’t seen much \(\texttt{antmaze}\) evaluation).</p>

<p>The DPO trick also brings other challenges for offline RL: in practice we want to use a Q function to estimate rewards with which the <strong>deadly triad</strong> can be notoriously unstable and overestimate values for OOD actions. We also need to approximate \(\pi_{\text{ref}}\) beyond the challenges of the quality of approximation, we do not know whether the behavior policy is multimodal (or how multimodal it is) which often necessitates VAEs (which produces explicit density estimates that can only be extracted by sampling) or MDNs (which have their own, set of training challenges + need to know how multimodal a behavior policy is beforehand).</p>

<p>Assuming that we have solved the aformentioned challenges, offline RL still poses yet another challenges: multimodal behavior policies can produce multimodal reward functions. DPO (and -like) objectives assume we can sample \(a_1, a_2\) such that \(a_1 \succ a_2\), but real-world offline datasets offer no such guarantee of paired-ness. Prior preference-based methods conventiently overcome this challenge by operating at the trajectory level or by using specific, preference-annotated datasets.</p>

<p>The key point I want to make in this, now long-winded post, is that the DPO tricks could enable a very interesting set of approaches to offline RL, provided we can overcome the fundamental challenges of value estimation, density estimation and limitations of on-policy evaluation.</p>

      </div>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#tech">tech</a>
          
            <a href="/tags#machine learning">machine learning</a>
          
            <a href="/tags#reinforcement learning">reinforcement learning</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2024-02-22-tech-blog-Some-Interesting-Offline-RL-Methods-Early-2024/" data-toggle="tooltip" data-placement="top" title="Some Interesting Offline RL Methods (Early 2024)">
            <i class="fas fa-arrow-left" alt="Previous Post"></i>
            <span class="d-none d-sm-inline-block">Previous Post</span>
          </a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2025-03-05-tech-blog-Behavioral-Supervisor-Tuning/" data-toggle="tooltip" data-placement="top" title="Intuition Explained: Behavioral Supervisor Tuning">
            <span class="d-none d-sm-inline-block">Next Post</span>
            <i class="fas fa-arrow-right" alt="Next Post"></i>
          </a>
        </li>
        
      </ul>
      
  
  
  

  


  

  



    </div>
  </div>
</main>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      
<ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:padmanabasrinivasan@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/dangerbot3pic" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/padmanaba-srinivasan" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://scholar.google.com/citations?user=citations?user=_yyqhBEAAAAJ&hl=en" title="Google Scholar">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Google Scholar</span>
    </a>
  </li></ul>


      
      <p class="copyright text-muted">
      
        Padmanaba Srinivasan
        &nbsp;&bull;&nbsp;
      
      2025

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4000/">padmanabasrinivasan.github.io</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
